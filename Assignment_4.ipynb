{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado\n",
    "\n",
    "1. Engineer your features. Here you do not have them for free. You need to think of possible ways for transforming the collected data into meaningful features. For some ideas, consider traditional features such as texture features, color features, bags of visual words or more powerful ones involving CNNs. If you cannot think of anything, talk to the professor for some ideas.\n",
    "\n",
    "2. Propose classification techniques to solve the problem. Suggestions here are the CNN directly, or SVMs/Random Forests allied with CNNs through the use of transfer learning.\n",
    "\n",
    "3. Consider using data augmentation in the training (what about in the testing as well?)\n",
    "\n",
    "4. Observation: You are free to use any solution to help you extract the features at this point.\n",
    "\n",
    "5. Report all of your results for the validation and test data. The labels for the test will be released one week before the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "\n",
    "1. LeNet-like\n",
    "    - Sem data augmentation com 50 epochs\n",
    "        - Acuracia-1 = 0.05463301346611636\n",
    "        - Acuracia-5 = 0.19096645689592612\n",
    "        - Acuracia normalizada = 0.055267239601399634\n",
    "        - F1 (media) = 0.0528899260412019\n",
    "    - Sem data augmentation com 35 epochs\n",
    "        - Acuracia-1 = 0.05596147537013693\n",
    "        - Acuracia-5 = 0.18764530092091783\n",
    "        - Acuracia normalizada = 0.05572421304755998\n",
    "        - F1 (media) = 0.049322308806100265\n",
    "    - Com data augmentation com 35 epochs \n",
    "        - Acuracia-1 = 0.056127533\n",
    "        - Acuracia-5 = 0.18067087\n",
    "        - Acuracia normalizada = 0.05669239630135797\n",
    "        - F1 (media) = 0.05433258611366018\n",
    "2. InceptionV3\n",
    "    - Treinando apenas FC (20 epochs)\n",
    "        - Acuracia-1 = 0.7789771\n",
    "        - Acuracia-5 = 0.95034873\n",
    "        - Acuracia normalizada = 0.7765120613992883\n",
    "        - F1 (media) = 0.7714184908455786\n",
    "     - teste\n",
    "     Acuracia-1: 0.8501845\n",
    "Acuracia-5: 0.98357934\n",
    "Acuracia normalizada: 0.8441153359252538\n",
    "F1 score: 0.8422229263930219"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import sklearn.metrics \n",
    "from keras import backend as K\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Flatten, Dense, Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from keras.utils import np_utils, Sequence\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Definicoes\n",
    "dataFolder = 'data/'\n",
    "trainFolder = dataFolder + 'train/'\n",
    "trainAugFolder = dataFolder + 'train_aug/'\n",
    "validationFolder = dataFolder + 'val/'\n",
    "testFolder = dataFolder + 'test/'\n",
    "\n",
    "numberOfClasses = 83\n",
    "largura = 200\n",
    "altura = 200\n",
    "profundidade = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Funcoes auxiliares\n",
    "\n",
    "#Retorna o vetor da imagem dado o nome do seu arquivo\n",
    "def le_imagem(name):\n",
    "    return io.imread(name,plugin='matplotlib') \n",
    "\n",
    "#Retorna o vetor da imagem e a classe dado o caminho do arquivo\n",
    "def le_imagem_e_classe(name):\n",
    "    img = le_imagem(name)\n",
    "    img = resize(img, (200, 200))\n",
    "    classe = np_utils.to_categorical(int(name.split('/')[2].split('_')[0]), numberOfClasses)\n",
    "    return img, classe\n",
    "\n",
    "#Retorna o vetor da imagem e a classe dado o caminho do arquivo\n",
    "def le_imagem_e_classe_aug(name):\n",
    "    img = le_imagem(name)\n",
    "    classe = np_utils.to_categorical(int(name.split('/')[2].split('_')[0]), numberOfClasses)\n",
    "    return img, classe\n",
    "\n",
    "def le_imagem_e_classe_test(name, lista):\n",
    "    img = le_imagem(name)\n",
    "    img = resize(img, (200, 200))\n",
    "    classe = np_utils.to_categorical(int(name.split('/')[2].split('_')[0]), numberOfClasses)\n",
    "    return img, classe\n",
    "\n",
    "#Retorna o caminho de todas as imagens dado a pasta\n",
    "def nome_das_imagens(pasta):\n",
    "    return glob(pasta + '*')\n",
    "\n",
    "#Retorna o numero correspondente a predicao\n",
    "def categorical_to_number(vector):\n",
    "    maior = 0\n",
    "    for i in range(len(vector)):\n",
    "        if vector[i] > vector[maior]:\n",
    "            maior = i\n",
    "    return maior\n",
    "\n",
    "def normalized_accuracy(y_true, y_pred):\n",
    "    acc_by_class = 0\n",
    "    ind_true = y_true.argmax(axis=1)\n",
    "    ind_pred = y_pred.argmax(axis=1)\n",
    "    for clss in range(numberOfClasses):\n",
    "        mask = (ind_true == clss)\n",
    "        acc_by_class += np.equal(ind_true[mask], ind_pred[mask]).mean()\n",
    "    \n",
    "    return acc_by_class/numberOfClasses\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_true = y_true.argmax(axis=1)\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "    return sklearn.metrics.f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "def metricas(y_true, y_pred):\n",
    "    tf = K.get_session()\n",
    "    #Retorna acuracias normal e top5\n",
    "    print(\"Acuracia-1: \" + str(tf.run(K.mean(categorical_accuracy(y_true, y_pred)))))\n",
    "    print(\"Acuracia-5: \" + str(tf.run(top_k_categorical_accuracy(y_true, y_pred))))\n",
    "\n",
    "    #Retorna nornmalized accuracy e F1\n",
    "    print(\"Acuracia normalizada: \" + str(normalized_accuracy(y_true, y_pred)))\n",
    "    print(\"F1 score: \" + str(f1(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200, 3)\n",
      "(33200, 200, 200, 3)\n",
      "(33200, 83)\n"
     ]
    }
   ],
   "source": [
    "#Le as imagens de treino\n",
    "X_train = []\n",
    "y_train = []\n",
    "for file in nome_das_imagens(trainFolder):\n",
    "    img, classe = le_imagem_e_classe(file)\n",
    "    X_train.append(img)\n",
    "    y_train.append(classe)\n",
    "\n",
    "#Le as imagens aumentadas tambem?\n",
    "if(True):\n",
    "    for file in nome_das_imagens(trainAugFolder):\n",
    "        img, classe = le_imagem_e_classe_aug(file)\n",
    "        X_train.append(img)\n",
    "        y_train.append(classe)\n",
    "    \n",
    "print(X_train[0].shape)\n",
    "X_train = np.array(X_train)\n",
    "print(X_train.shape)\n",
    "y_train = np.array(y_train)\n",
    "print(y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200, 3)\n",
      "(6022, 200, 200, 3)\n",
      "(6022, 83)\n"
     ]
    }
   ],
   "source": [
    "#Le as imagens de validacao\n",
    "X_val = []\n",
    "y_val = []\n",
    "for file in nome_das_imagens(validationFolder):\n",
    "    img, classe = le_imagem_e_classe(file)\n",
    "    X_val.append(img)\n",
    "    y_val.append(classe)\n",
    "\n",
    "print(X_val[0].shape)\n",
    "X_val = np.array(X_val)\n",
    "print(X_val.shape)\n",
    "y_val = np.array(y_val)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200, 3)\n",
      "(5420, 200, 200, 3)\n",
      "(5420, 83)\n"
     ]
    }
   ],
   "source": [
    "#Le as imagens de test\n",
    "X_test = []\n",
    "y_test = []\n",
    "lista = open('MO444_dogs_test.txt', 'r').readlines()\n",
    "\n",
    "for line in range(len(lista)):\n",
    "    lista[line] = (lista[line].split(' ')[1].split('/')[-1], lista[line].split(' ')[2])\n",
    "#print(lista)                   \n",
    "for file in lista:\n",
    "    img = le_imagem(testFolder + file[0])\n",
    "    img = resize(img, (200, 200))\n",
    "    X_test.append(img)\n",
    "    y_test.append(np_utils.to_categorical(int(file[1]), numberOfClasses))\n",
    "\n",
    "print(X_test[0].shape)\n",
    "X_test = np.array(X_test)\n",
    "print(X_test.shape)\n",
    "y_test = np.array(y_test)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lenet\n",
    "\n",
    "class LeNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes, weightsPath=None):\n",
    "        # Inicializa modelo\n",
    "        model = Sequential()\n",
    "        initial_shape = (height, width, depth)\n",
    "        \n",
    "        # 1a layer CONV => RELU => POOL\n",
    "        model.add(Conv2D(filters=6, kernel_size=5, strides=1, input_shape=initial_shape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "                  \n",
    "        # 2a layer CONV => RELU => POOL\n",
    "        model.add(Conv2D(filters=16, kernel_size=5, strides=1))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "                  \n",
    "        # FC layer\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(120))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(84))\n",
    "        model.add(Activation(\"relu\"))\n",
    "                  \n",
    "        # classificador softmax \n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "                  \n",
    "        if weightsPath is not None:\n",
    "            model.load_weights(weightsPath)\n",
    " \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### LeNet sem data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Treinando sem data augmentation\n",
    "model = LeNet.build(width=largura, height=altura, depth=profundidade, classes=numberOfClasses, \\\n",
    "                    weightsPath=None)\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, \\\n",
    "              metrics=[categorical_accuracy, top_k_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gerar imagens da arquitetura da rede\n",
    "plot_model(model, to_file='lenet.png', show_shapes=True, show_layer_names=False)\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.4155 - acc: 0.0154\n",
      "Epoch 2/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.3926 - acc: 0.0186\n",
      "Epoch 3/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.3677 - acc: 0.0202\n",
      "Epoch 4/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.3425 - acc: 0.0212\n",
      "Epoch 5/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.3162 - acc: 0.0295\n",
      "Epoch 6/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.2861 - acc: 0.0325\n",
      "Epoch 7/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.2556 - acc: 0.0357\n",
      "Epoch 8/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.2279 - acc: 0.0365\n",
      "Epoch 9/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.1968 - acc: 0.0402\n",
      "Epoch 10/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.1666 - acc: 0.0445\n",
      "Epoch 11/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.1357 - acc: 0.0518\n",
      "Epoch 12/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.1106 - acc: 0.0520\n",
      "Epoch 13/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.0832 - acc: 0.0575\n",
      "Epoch 14/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.0562 - acc: 0.0633\n",
      "Epoch 15/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 4.0179 - acc: 0.0682\n",
      "Epoch 16/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 3.9688 - acc: 0.0759\n",
      "Epoch 17/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 3.9274 - acc: 0.0813\n",
      "Epoch 18/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 3.8787 - acc: 0.0916\n",
      "Epoch 19/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 3.8156 - acc: 0.1041\n",
      "Epoch 20/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 3.7414 - acc: 0.1139\n",
      "Epoch 21/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 3.6631 - acc: 0.1242\n",
      "Epoch 22/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 3.5686 - acc: 0.1467\n",
      "Epoch 23/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 3.4587 - acc: 0.1763\n",
      "Epoch 24/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 3.2990 - acc: 0.2045\n",
      "Epoch 25/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 3.1155 - acc: 0.2377\n",
      "Epoch 26/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 2.9607 - acc: 0.2634\n",
      "Epoch 27/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 2.7238 - acc: 0.3222\n",
      "Epoch 28/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 2.4534 - acc: 0.3867\n",
      "Epoch 29/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 2.1137 - acc: 0.4594\n",
      "Epoch 30/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 1.8489 - acc: 0.5195\n",
      "Epoch 31/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 1.4541 - acc: 0.6163\n",
      "Epoch 32/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 1.0197 - acc: 0.7369\n",
      "Epoch 33/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.7016 - acc: 0.8231\n",
      "Epoch 34/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.3763 - acc: 0.9163\n",
      "Epoch 35/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.1605 - acc: 0.9772\n",
      "Epoch 36/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0766 - acc: 0.9953\n",
      "Epoch 37/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0452 - acc: 0.9983\n",
      "Epoch 38/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0320 - acc: 0.9988\n",
      "Epoch 39/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0263 - acc: 0.9992\n",
      "Epoch 40/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0208 - acc: 0.9992\n",
      "Epoch 41/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0169 - acc: 0.9993\n",
      "Epoch 42/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0176 - acc: 0.9993\n",
      "Epoch 43/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0169 - acc: 0.9992\n",
      "Epoch 44/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0146 - acc: 0.9994\n",
      "Epoch 45/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0166 - acc: 0.9990\n",
      "Epoch 46/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0127 - acc: 0.9993\n",
      "Epoch 47/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0115 - acc: 0.9992\n",
      "Epoch 48/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0113 - acc: 0.9993\n",
      "Epoch 49/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0089 - acc: 0.9995\n",
      "Epoch 50/50\n",
      "8300/8300 [==============================] - 11s 1ms/step - loss: 0.0118 - acc: 0.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f58a08198>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train, batch_size=100, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia-1: 0.054633014\n",
      "Acuracia-5: 0.19096646\n",
      "Acuracia normalizada: 0.055267239601399634\n",
      "F1 score: 0.0528899260412019\n"
     ]
    }
   ],
   "source": [
    "#Testa o modelo treinado \n",
    "\n",
    "pred = model.predict(X_val, batch_size=500)\n",
    "metricas(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Guarda os pesos treinados\n",
    "model.save_weights('lenet_nda_50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float32 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    }
   ],
   "source": [
    "#Data augmentation\n",
    "data_aug_gen = ImageDataGenerator(rotation_range=30, width_shift_range=0.2, height_shift_range=0.2, \\\n",
    "                                  shear_range=0.2, zoom_range=0.2, vertical_flip=True)\n",
    "\n",
    "iterator = data_aug_gen.flow(X_train, y_train, batch_size=100, seed=1)\n",
    "\n",
    "\n",
    "#Geracao\n",
    "i = 0\n",
    "for batch in iterator:\n",
    "    if(i == 24900):\n",
    "        break\n",
    "    imgs = batch[0]\n",
    "    labels = batch[1].argmax(axis=1)\n",
    "    for ind in range(imgs.shape[0]):\n",
    "        io.imsave('data/train_aug/' + str(labels[ind]) + '_' + str(i) + '.png', imgs[ind,:,:,:])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### LeNet com data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Treinando com data augmentation\n",
    "model = LeNet.build(width=largura, height=altura, depth=profundidade, classes=numberOfClasses, \\\n",
    "                    weightsPath='lenet_cda_35.h5')\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, \\\n",
    "              metrics=[categorical_accuracy, top_k_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "33200/33200 [==============================] - 48s 1ms/step - loss: 4.4146 - categorical_accuracy: 0.0133 - top_k_categorical_accuracy: 0.0673\n",
      "Epoch 2/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 4.3946 - categorical_accuracy: 0.0176 - top_k_categorical_accuracy: 0.0843\n",
      "Epoch 3/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 4.3617 - categorical_accuracy: 0.0228 - top_k_categorical_accuracy: 0.0989\n",
      "Epoch 4/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 4.3098 - categorical_accuracy: 0.0301 - top_k_categorical_accuracy: 0.1257\n",
      "Epoch 5/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 4.2363 - categorical_accuracy: 0.0365 - top_k_categorical_accuracy: 0.1508\n",
      "Epoch 6/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 4.1636 - categorical_accuracy: 0.0422 - top_k_categorical_accuracy: 0.1729\n",
      "Epoch 7/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 4.1131 - categorical_accuracy: 0.0482 - top_k_categorical_accuracy: 0.1916\n",
      "Epoch 8/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 4.0755 - categorical_accuracy: 0.0530 - top_k_categorical_accuracy: 0.2054\n",
      "Epoch 9/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 4.0423 - categorical_accuracy: 0.0584 - top_k_categorical_accuracy: 0.2170\n",
      "Epoch 10/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 4.0050 - categorical_accuracy: 0.0651 - top_k_categorical_accuracy: 0.2345\n",
      "Epoch 11/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 3.9581 - categorical_accuracy: 0.0722 - top_k_categorical_accuracy: 0.2540\n",
      "Epoch 12/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 3.9064 - categorical_accuracy: 0.0828 - top_k_categorical_accuracy: 0.2741\n",
      "Epoch 13/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 3.8442 - categorical_accuracy: 0.0918 - top_k_categorical_accuracy: 0.2943\n",
      "Epoch 14/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 3.7680 - categorical_accuracy: 0.1067 - top_k_categorical_accuracy: 0.3194\n",
      "Epoch 15/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 3.6848 - categorical_accuracy: 0.1213 - top_k_categorical_accuracy: 0.3509\n",
      "Epoch 16/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 3.5708 - categorical_accuracy: 0.1411 - top_k_categorical_accuracy: 0.3866\n",
      "Epoch 17/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 3.4461 - categorical_accuracy: 0.1644 - top_k_categorical_accuracy: 0.4253\n",
      "Epoch 18/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 3.2844 - categorical_accuracy: 0.1947 - top_k_categorical_accuracy: 0.4732\n",
      "Epoch 19/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 3.1127 - categorical_accuracy: 0.2311 - top_k_categorical_accuracy: 0.5176\n",
      "Epoch 20/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 2.9038 - categorical_accuracy: 0.2728 - top_k_categorical_accuracy: 0.5745\n",
      "Epoch 21/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 2.6701 - categorical_accuracy: 0.3216 - top_k_categorical_accuracy: 0.6312\n",
      "Epoch 22/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 2.4054 - categorical_accuracy: 0.3800 - top_k_categorical_accuracy: 0.6900\n",
      "Epoch 23/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 2.1256 - categorical_accuracy: 0.4432 - top_k_categorical_accuracy: 0.7481\n",
      "Epoch 24/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 1.8445 - categorical_accuracy: 0.5078 - top_k_categorical_accuracy: 0.8017\n",
      "Epoch 25/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 1.5297 - categorical_accuracy: 0.5886 - top_k_categorical_accuracy: 0.8524\n",
      "Epoch 26/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 1.2601 - categorical_accuracy: 0.6523 - top_k_categorical_accuracy: 0.8947\n",
      "Epoch 27/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 0.9926 - categorical_accuracy: 0.7257 - top_k_categorical_accuracy: 0.9294\n",
      "Epoch 28/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 0.7606 - categorical_accuracy: 0.7897 - top_k_categorical_accuracy: 0.9546\n",
      "Epoch 29/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 0.5523 - categorical_accuracy: 0.8465 - top_k_categorical_accuracy: 0.9759\n",
      "Epoch 30/35\n",
      "33200/33200 [==============================] - 47s 1ms/step - loss: 0.4245 - categorical_accuracy: 0.8814 - top_k_categorical_accuracy: 0.9853\n",
      "Epoch 31/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 0.3138 - categorical_accuracy: 0.9164 - top_k_categorical_accuracy: 0.9918\n",
      "Epoch 32/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 0.1970 - categorical_accuracy: 0.9495 - top_k_categorical_accuracy: 0.9970\n",
      "Epoch 33/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 0.0904 - categorical_accuracy: 0.9828 - top_k_categorical_accuracy: 0.9991\n",
      "Epoch 34/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 0.0346 - categorical_accuracy: 0.9965 - top_k_categorical_accuracy: 0.9998\n",
      "Epoch 35/35\n",
      "33200/33200 [==============================] - 46s 1ms/step - loss: 0.0144 - categorical_accuracy: 0.9994 - top_k_categorical_accuracy: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff31983b080>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train, batch_size=100, epochs=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia-1: 0.056127533\n",
      "Acuracia-5: 0.18067087\n",
      "Acuracia normalizada: 0.05669239630135797\n",
      "F1 score: 0.05433258611366018\n"
     ]
    }
   ],
   "source": [
    "#Testa o modelo treinado \n",
    "\n",
    "pred = model.predict(X_val, batch_size=500)\n",
    "metricas(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Guarda os pesos treinados\n",
    "model.save_weights('lenet_cda_35.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### InceptionV3 congelando todas as layers convolucionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criando modelo\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(200,200,3))\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "predictions = Dense(numberOfClasses, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#Freezing layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.load_weights('inceptionv3_cda_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gerar imagens da arquitetura da rede\n",
    "plot_model(model, to_file='inceptionv3.png', show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compilar\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, \\\n",
    "              metrics=[categorical_accuracy, top_k_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33200/33200 [==============================] - 163s 5ms/step - loss: 3.3679 - categorical_accuracy: 0.2707 - top_k_categorical_accuracy: 0.4912\n",
      "Epoch 2/20\n",
      "33200/33200 [==============================] - 160s 5ms/step - loss: 2.4245 - categorical_accuracy: 0.4201 - top_k_categorical_accuracy: 0.6792\n",
      "Epoch 3/20\n",
      "33200/33200 [==============================] - 160s 5ms/step - loss: 2.1666 - categorical_accuracy: 0.4579 - top_k_categorical_accuracy: 0.7282\n",
      "Epoch 4/20\n",
      "33200/33200 [==============================] - 160s 5ms/step - loss: 2.0207 - categorical_accuracy: 0.4892 - top_k_categorical_accuracy: 0.7573\n",
      "Epoch 5/20\n",
      "33200/33200 [==============================] - 160s 5ms/step - loss: 1.9147 - categorical_accuracy: 0.5105 - top_k_categorical_accuracy: 0.7770\n",
      "Epoch 6/20\n",
      "33200/33200 [==============================] - 160s 5ms/step - loss: 1.8338 - categorical_accuracy: 0.5255 - top_k_categorical_accuracy: 0.7913\n",
      "Epoch 7/20\n",
      "33200/33200 [==============================] - 160s 5ms/step - loss: 1.7644 - categorical_accuracy: 0.5394 - top_k_categorical_accuracy: 0.8059\n",
      "Epoch 8/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.7127 - categorical_accuracy: 0.5536 - top_k_categorical_accuracy: 0.8142\n",
      "Epoch 9/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.6607 - categorical_accuracy: 0.5661 - top_k_categorical_accuracy: 0.8248\n",
      "Epoch 10/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.6207 - categorical_accuracy: 0.5710 - top_k_categorical_accuracy: 0.8336\n",
      "Epoch 11/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.5743 - categorical_accuracy: 0.5820 - top_k_categorical_accuracy: 0.8389\n",
      "Epoch 12/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.5419 - categorical_accuracy: 0.5890 - top_k_categorical_accuracy: 0.8439\n",
      "Epoch 13/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.5048 - categorical_accuracy: 0.5967 - top_k_categorical_accuracy: 0.8525\n",
      "Epoch 14/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.4741 - categorical_accuracy: 0.6057 - top_k_categorical_accuracy: 0.8563\n",
      "Epoch 15/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.4482 - categorical_accuracy: 0.6125 - top_k_categorical_accuracy: 0.8611\n",
      "Epoch 16/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.4114 - categorical_accuracy: 0.6216 - top_k_categorical_accuracy: 0.8675\n",
      "Epoch 17/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.3854 - categorical_accuracy: 0.6269 - top_k_categorical_accuracy: 0.8720\n",
      "Epoch 18/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.3655 - categorical_accuracy: 0.6313 - top_k_categorical_accuracy: 0.8754\n",
      "Epoch 19/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.3310 - categorical_accuracy: 0.6384 - top_k_categorical_accuracy: 0.8805\n",
      "Epoch 20/20\n",
      "33200/33200 [==============================] - 161s 5ms/step - loss: 1.3075 - categorical_accuracy: 0.6425 - top_k_categorical_accuracy: 0.8841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f837e5bbb38>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treina o modelo\n",
    "model.fit(X_train,y_train, batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Salva os pesos\n",
    "model.save_weights('inceptionv3_cda_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia-1: 0.8501845\n",
      "Acuracia-5: 0.98357934\n",
      "Acuracia normalizada: 0.8441153359252538\n",
      "F1 score: 0.8422229263930219\n"
     ]
    }
   ],
   "source": [
    "#Testa o modelo treinado \n",
    "\n",
    "pred = model.predict(X_test, batch_size=500)\n",
    "metricas(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### InceptionV3 congelando todas as layers convolucionais menos 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando modelo\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(200,200,3))\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "predictions = Dense(numberOfClasses, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Congelando todas as layers menos as duas ultimas\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compilar\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, \\\n",
    "              metrics=[categorical_accuracy, top_k_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33200/33200 [==============================] - 179s 5ms/step - loss: 3.3716 - categorical_accuracy: 0.2876 - top_k_categorical_accuracy: 0.4959\n",
      "Epoch 2/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 2.0563 - categorical_accuracy: 0.5155 - top_k_categorical_accuracy: 0.7755\n",
      "Epoch 3/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 1.5202 - categorical_accuracy: 0.6044 - top_k_categorical_accuracy: 0.8516\n",
      "Epoch 4/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 1.1996 - categorical_accuracy: 0.6827 - top_k_categorical_accuracy: 0.8989\n",
      "Epoch 5/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.9494 - categorical_accuracy: 0.7510 - top_k_categorical_accuracy: 0.9330\n",
      "Epoch 6/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.7284 - categorical_accuracy: 0.8125 - top_k_categorical_accuracy: 0.9597\n",
      "Epoch 7/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.5404 - categorical_accuracy: 0.8701 - top_k_categorical_accuracy: 0.9777\n",
      "Epoch 8/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.3912 - categorical_accuracy: 0.9133 - top_k_categorical_accuracy: 0.9892\n",
      "Epoch 9/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.2763 - categorical_accuracy: 0.9482 - top_k_categorical_accuracy: 0.9947\n",
      "Epoch 10/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.1930 - categorical_accuracy: 0.9702 - top_k_categorical_accuracy: 0.9976\n",
      "Epoch 11/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.1435 - categorical_accuracy: 0.9815 - top_k_categorical_accuracy: 0.9991\n",
      "Epoch 12/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.1056 - categorical_accuracy: 0.9886 - top_k_categorical_accuracy: 0.9997\n",
      "Epoch 13/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.0818 - categorical_accuracy: 0.9930 - top_k_categorical_accuracy: 0.9999\n",
      "Epoch 14/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.0631 - categorical_accuracy: 0.9960 - top_k_categorical_accuracy: 0.9999\n",
      "Epoch 15/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.0516 - categorical_accuracy: 0.9970 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.0447 - categorical_accuracy: 0.9973 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.0370 - categorical_accuracy: 0.9980 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "33200/33200 [==============================] - 171s 5ms/step - loss: 0.0325 - categorical_accuracy: 0.9986 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.0294 - categorical_accuracy: 0.9982 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "33200/33200 [==============================] - 172s 5ms/step - loss: 0.0253 - categorical_accuracy: 0.9991 - top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f972dcf1a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treina o modelo\n",
    "model.fit(X_train,y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Salva os pesos\n",
    "model.save_weights('inceptionv3_fino_cda_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia-1: 0.74958485\n",
      "Acuracia-5: 0.9402192\n",
      "Acuracia normalizada: 0.744914899793834\n",
      "F1 score: 0.7413158812959674\n"
     ]
    }
   ],
   "source": [
    "#Testa o modelo treinado \n",
    "\n",
    "pred = model.predict(X_val, batch_size=500)\n",
    "metricas(y_val, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
